{"ast":null,"code":"import natural from 'natural';\nimport { findSynonyms } from './textProcessing';\nconst tokenizer = new natural.WordTokenizer();\nconst sentenceTokenizer = new natural.SentenceTokenizer();\nexport const optimizeResume = async (resumeText, jobKeywords) => {\n  const sentences = sentenceTokenizer.tokenize(resumeText);\n  const optimizedSentences = [];\n  let totalWordCount = 0;\n  const originalWordCount = tokenizer.tokenize(resumeText).length;\n  const maxWordIncrease = 30;\n  for (const sentence of sentences) {\n    const words = tokenizer.tokenize(sentence);\n    const optimizedWords = [...words];\n    let modified = false;\n\n    // Check each word in the sentence\n    for (let i = 0; i < words.length; i++) {\n      const word = words[i].toLowerCase();\n\n      // If the word is in job keywords, try to find a better synonym\n      if (jobKeywords.includes(word)) {\n        const synonyms = findSynonyms(word);\n        if (synonyms.length > 0) {\n          // Choose a synonym that better matches the job description\n          const bestSynonym = synonyms[0]; // In a real application, you might want to choose the best synonym based on context\n          optimizedWords[i] = bestSynonym;\n          modified = true;\n        }\n      }\n    }\n\n    // Only include modified sentences if we haven't exceeded the word limit\n    const newWordCount = tokenizer.tokenize(optimizedWords.join(' ')).length;\n    if (modified && totalWordCount + newWordCount - words.length <= originalWordCount + maxWordIncrease) {\n      optimizedSentences.push(optimizedWords.join(' '));\n      totalWordCount += newWordCount - words.length;\n    } else {\n      optimizedSentences.push(sentence);\n    }\n  }\n  return optimizedSentences.join(' ');\n};\nexport const validateOptimization = (originalText, optimizedText) => {\n  const originalWords = tokenizer.tokenize(originalText).length;\n  const optimizedWords = tokenizer.tokenize(optimizedText).length;\n  if (optimizedWords > originalWords + 30) {\n    throw new Error('Optimization exceeded maximum word count increase');\n  }\n  return true;\n};","map":{"version":3,"names":["natural","findSynonyms","tokenizer","WordTokenizer","sentenceTokenizer","SentenceTokenizer","optimizeResume","resumeText","jobKeywords","sentences","tokenize","optimizedSentences","totalWordCount","originalWordCount","length","maxWordIncrease","sentence","words","optimizedWords","modified","i","word","toLowerCase","includes","synonyms","bestSynonym","newWordCount","join","push","validateOptimization","originalText","optimizedText","originalWords","Error"],"sources":["/var/home/dadmin/hackapp/src/utils/optimization.js"],"sourcesContent":["import natural from 'natural';\nimport { findSynonyms } from './textProcessing';\n\nconst tokenizer = new natural.WordTokenizer();\nconst sentenceTokenizer = new natural.SentenceTokenizer();\n\nexport const optimizeResume = async (resumeText, jobKeywords) => {\n  const sentences = sentenceTokenizer.tokenize(resumeText);\n  const optimizedSentences = [];\n  let totalWordCount = 0;\n  const originalWordCount = tokenizer.tokenize(resumeText).length;\n  const maxWordIncrease = 30;\n\n  for (const sentence of sentences) {\n    const words = tokenizer.tokenize(sentence);\n    const optimizedWords = [...words];\n    let modified = false;\n\n    // Check each word in the sentence\n    for (let i = 0; i < words.length; i++) {\n      const word = words[i].toLowerCase();\n      \n      // If the word is in job keywords, try to find a better synonym\n      if (jobKeywords.includes(word)) {\n        const synonyms = findSynonyms(word);\n        if (synonyms.length > 0) {\n          // Choose a synonym that better matches the job description\n          const bestSynonym = synonyms[0]; // In a real application, you might want to choose the best synonym based on context\n          optimizedWords[i] = bestSynonym;\n          modified = true;\n        }\n      }\n    }\n\n    // Only include modified sentences if we haven't exceeded the word limit\n    const newWordCount = tokenizer.tokenize(optimizedWords.join(' ')).length;\n    if (modified && (totalWordCount + newWordCount - words.length) <= originalWordCount + maxWordIncrease) {\n      optimizedSentences.push(optimizedWords.join(' '));\n      totalWordCount += newWordCount - words.length;\n    } else {\n      optimizedSentences.push(sentence);\n    }\n  }\n\n  return optimizedSentences.join(' ');\n};\n\nexport const validateOptimization = (originalText, optimizedText) => {\n  const originalWords = tokenizer.tokenize(originalText).length;\n  const optimizedWords = tokenizer.tokenize(optimizedText).length;\n  \n  if (optimizedWords > originalWords + 30) {\n    throw new Error('Optimization exceeded maximum word count increase');\n  }\n\n  return true;\n}; "],"mappings":"AAAA,OAAOA,OAAO,MAAM,SAAS;AAC7B,SAASC,YAAY,QAAQ,kBAAkB;AAE/C,MAAMC,SAAS,GAAG,IAAIF,OAAO,CAACG,aAAa,CAAC,CAAC;AAC7C,MAAMC,iBAAiB,GAAG,IAAIJ,OAAO,CAACK,iBAAiB,CAAC,CAAC;AAEzD,OAAO,MAAMC,cAAc,GAAG,MAAAA,CAAOC,UAAU,EAAEC,WAAW,KAAK;EAC/D,MAAMC,SAAS,GAAGL,iBAAiB,CAACM,QAAQ,CAACH,UAAU,CAAC;EACxD,MAAMI,kBAAkB,GAAG,EAAE;EAC7B,IAAIC,cAAc,GAAG,CAAC;EACtB,MAAMC,iBAAiB,GAAGX,SAAS,CAACQ,QAAQ,CAACH,UAAU,CAAC,CAACO,MAAM;EAC/D,MAAMC,eAAe,GAAG,EAAE;EAE1B,KAAK,MAAMC,QAAQ,IAAIP,SAAS,EAAE;IAChC,MAAMQ,KAAK,GAAGf,SAAS,CAACQ,QAAQ,CAACM,QAAQ,CAAC;IAC1C,MAAME,cAAc,GAAG,CAAC,GAAGD,KAAK,CAAC;IACjC,IAAIE,QAAQ,GAAG,KAAK;;IAEpB;IACA,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGH,KAAK,CAACH,MAAM,EAAEM,CAAC,EAAE,EAAE;MACrC,MAAMC,IAAI,GAAGJ,KAAK,CAACG,CAAC,CAAC,CAACE,WAAW,CAAC,CAAC;;MAEnC;MACA,IAAId,WAAW,CAACe,QAAQ,CAACF,IAAI,CAAC,EAAE;QAC9B,MAAMG,QAAQ,GAAGvB,YAAY,CAACoB,IAAI,CAAC;QACnC,IAAIG,QAAQ,CAACV,MAAM,GAAG,CAAC,EAAE;UACvB;UACA,MAAMW,WAAW,GAAGD,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC;UACjCN,cAAc,CAACE,CAAC,CAAC,GAAGK,WAAW;UAC/BN,QAAQ,GAAG,IAAI;QACjB;MACF;IACF;;IAEA;IACA,MAAMO,YAAY,GAAGxB,SAAS,CAACQ,QAAQ,CAACQ,cAAc,CAACS,IAAI,CAAC,GAAG,CAAC,CAAC,CAACb,MAAM;IACxE,IAAIK,QAAQ,IAAKP,cAAc,GAAGc,YAAY,GAAGT,KAAK,CAACH,MAAM,IAAKD,iBAAiB,GAAGE,eAAe,EAAE;MACrGJ,kBAAkB,CAACiB,IAAI,CAACV,cAAc,CAACS,IAAI,CAAC,GAAG,CAAC,CAAC;MACjDf,cAAc,IAAIc,YAAY,GAAGT,KAAK,CAACH,MAAM;IAC/C,CAAC,MAAM;MACLH,kBAAkB,CAACiB,IAAI,CAACZ,QAAQ,CAAC;IACnC;EACF;EAEA,OAAOL,kBAAkB,CAACgB,IAAI,CAAC,GAAG,CAAC;AACrC,CAAC;AAED,OAAO,MAAME,oBAAoB,GAAGA,CAACC,YAAY,EAAEC,aAAa,KAAK;EACnE,MAAMC,aAAa,GAAG9B,SAAS,CAACQ,QAAQ,CAACoB,YAAY,CAAC,CAAChB,MAAM;EAC7D,MAAMI,cAAc,GAAGhB,SAAS,CAACQ,QAAQ,CAACqB,aAAa,CAAC,CAACjB,MAAM;EAE/D,IAAII,cAAc,GAAGc,aAAa,GAAG,EAAE,EAAE;IACvC,MAAM,IAAIC,KAAK,CAAC,mDAAmD,CAAC;EACtE;EAEA,OAAO,IAAI;AACb,CAAC","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}